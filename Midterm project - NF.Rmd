---
title: "Midterm project - Nathalie"
author: "Nathalie Fadel"
date: "4/4/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(caret)
library(splines)
library(mgcv)
library(ISLR)
library(pastecs)
library(knitr)
library(rpart)
library(rpart.plot)
library(caTools)

```

###Import & tidy dataset
```{r}
re_data = read_excel("real_estate_valuation_data_set.xlsx") %>%
  janitor::clean_names() %>%
  select(-no)

re_data = 
  re_data %>%
  rename(house_price = y_house_price_of_unit_area, 
         transaction_date = x1_transaction_date,
         house_age = x2_house_age,
         distance_mrt = x3_distance_to_the_nearest_mrt_station,
         conv_stores = x4_number_of_convenience_stores,
         latitude = x5_latitude,
         longitude = x6_longitude
         )
#renamed variables

#Descriptive Statistics
summary(re_data)

```

###Scatter Plot
```{r}
data(re_data)
# matrix of predictors 
x <- model.matrix(house_price~.,re_data)[,-1]
# vector of response
y <- re_data$house_price

theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(4, 2))

```
We can see that there may be a non-linear relationship between house price and distance to MRT station, and between house price and house age. We will test both of these terms independently and together as spline terms in GAM models.

###GAM models
```{r}
set.seed(123)
#Spliting data as training and test set. Using createDataPartition() function from caret
indxTrain <- createDataPartition(y = re_data$house_price, p = 0.80, list = FALSE)
training <- re_data[indxTrain,]
testing <- re_data[-indxTrain,]

gam.m1 <- gam(house_price ~ transaction_date + house_age + distance_mrt + conv_stores + latitude + longitude, data = training)

gam.m2 <- gam(house_price ~ transaction_date + house_age + s(distance_mrt) + conv_stores + latitude + longitude, data = training)
#Spline term applied to distance to mrt station

gam.m3 <- gam(house_price ~ transaction_date + s(house_age) + distance_mrt + conv_stores + latitude + longitude, data = training)
#spline term applied to house age

gam.m4 <- gam(house_price ~ transaction_date + s(house_age) + s(distance_mrt) + conv_stores + latitude + longitude, data = training)
#Both house age and distance to mrt station are splined

anova(gam.m1, gam.m2, gam.m3, gam.m4, test = "F")

summary(gam.m2)
summary(gam.m3)
summary(gam.m4)
#GAM M4 has best R^2 value

plot(gam.m2)
plot(gam.m3)

p.m4 = predict(gam.m4, newdata = testing)
RMSE(p.m4, testing$house_price) 

```  

###K-Nearest Neighbors fit
```{r}
set.seed(123)
#Spliting data as training and test set. Using createDataPartition() function from caret
indxTrain <- createDataPartition(y = re_data$house_price, p = 0.80, list = FALSE)
training <- re_data[indxTrain,]
testing <- re_data[-indxTrain,]

dim(training); dim(testing);
#train: 332, 7
#test: 82, 7

set.seed(1)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
knn_fit <- train(house_price ~., data = training, method = "knn",
            trControl = trctrl,
            preProcess = c("center", "scale"),
            tuneLength = 10)
knn_fit

# Plot model error RMSE vs different values of k
ggplot(knn_fit)
# Best tuning parameter k that minimizes the RMSE
knn_fit$bestTune
# Make predictions on the test data
knn_predict <- knn_fit %>% predict(testing)
# Compute the prediction error RMSE
RMSE(knn_predict, testing$house_price)

```


#Regression Trees
```{r}
#using caTools
set.seed(1)
split = sample.split(re_data$house_price, SplitRatio = 0.8)
train = subset(re_data, split==TRUE)
test = subset(re_data, split==FALSE)

#using rpart
set.seed(1)
tree1 <- rpart(formula = house_price~., data = train)

plotcp(tree1)
rpart.plot(tree1)
cpTable <- printcp(tree1)

minErr <- which.min(cpTable[,5])
#prune tree
tree2 <- prune(tree1, cpTable[minErr,1]) #prune based on minimum cv error
rpart.plot(tree2)
plotcp(tree2)

tree.pred = predict(tree2, newdata = test)
RMSE(tree.pred, test$house_price)

```

#Ensemble methods - random forests and boosting
```{r}


```
